# -*- coding: utf-8 -*-
#/usr/bin/python2

'''
As vctk_01, but adding speaker codes in 2 locations: 'text_encoder_towards_end', 'audio_decoder_input'
'''


import os


## Take name of this file to be the config name:
config_name = os.path.split(__file__)[-1].replace('.cfg', '')  ## remove path and extension

## Define place to put outputs relative to this config file's location;
## supply an absoluate path to work elsewhere:
topworkdir = os.path.realpath(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'work')))

voicedir = os.path.join(topworkdir, config_name)
logdir = os.path.join(voicedir, 'train')
sampledir = os.path.join(voicedir, 'synth')

## Change featuredir to absolute path to use existing features
featuredir = os.path.join(topworkdir, 'vctk_01', 'data')  ## use older data
coarse_audio_dir = os.path.join(featuredir, 'mels')
full_mel_dir = os.path.join(featuredir, 'full_mels')
full_audio_dir = os.path.join(featuredir, 'mags')
attention_guide_dir = os.path.join(featuredir, 'attention_guides')  ## Set this to the empty string ('') to global attention guide



# data locations:
datadir = '/group/project/cstr2/owatts/data/VCTK-Corpus/' 


transcript = os.path.join(datadir, 'transcript.csv')
test_transcript = os.path.join(datadir, 'harvard_combilex_rpx.csv')
waveforms = os.path.join(datadir, 'wav_trim_15dB')



input_type = 'phones' ## letters or phones
## Combilex RPX plus extra symbols:- 
vocab = ['<PADDING>', '<!>', "<'>", "<'s>", '<)>', '<]>', '<">', '<,>', '<.>', '<:>', '<;>', '<>', '<?>', \
         '<_END_>', '<_START_>', \
         '@', '@@', '@U', 'A', 'D', 'E', 'E@', 'I', 'I@', 'N', 'O', 'OI', 'Q', 'S', 'T', 'U',\
         'U@', 'V', 'Z', 'a', 'aI', 'aU', 'b', 'd', 'dZ', 'eI', 'f', 'g', 'h', 'i', 'j', 'k',\
         'l', 'l!', 'lw', 'm', 'm!', 'n', 'n!', 'o^', 'o~', 'p', 'r', 's', 't', 'tS', 'u', 'v', 'w', 'z']

### CMU lex version:-
# vocab = ['<PADDING>', '<!>', '<">', "<'>", "<'s>", '<)>', '<,>', '<.>', '<:>', \
#             '<;>', '<>', '<?>', '<]>', '<_END_>', '<_START_>', 'aa', 'ae', 'ah', \
#             'ao', 'aw', 'ax', 'ay', 'b', 'ch', 'd', 'dh', 'eh', 'er', 'ey', \
#             'f', 'g', 'hh', 'ih', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', \
#             'oy', 'p', 'pau', 'r', 's', 'sh', 't', 'th', 'uh', 'uw', 'v', \
#             'w', 'y', 'z', 'zh']

#vocab = "PE abcdefghijklmnopqrstuvwxyz'.?" # P: Padding, E: EOS.
max_N = 80 # Maximum number of characters/phones #
max_T = 100 # Maximum number of mel frames. #
multispeaker = ['text_encoder_towards_end', 'audio_decoder_input'] ## []: speaker dependent. text_encoder_input, text_encoder_towards_end, audio_decoder_input, ssrn_input, audio_encoder_input
speaker_list = ['<PADDING>']  +  ['p225', 'p226', 'p227', 'p228', 'p229', 'p230', 'p231', 'p232', 'p233', 'p234', 'p236', 'p237', 'p238', 'p239', 'p240', 'p241', 'p243', 'p244', 'p245', 'p246', 'p247', 'p248', 'p249', 'p250', 'p251', 'p252', 'p253', 'p254', 'p255', 'p256', 'p257', 'p258', 'p259', 'p260', 'p261', 'p262', 'p263', 'p264', 'p265', 'p266', 'p267', 'p268', 'p269', 'p270', 'p271', 'p272', 'p273', 'p274', 'p275', 'p276', 'p277', 'p278', 'p279', 'p280', 'p281', 'p282', 'p283', 'p284', 'p285', 'p286', 'p287', 'p288', 'p292', 'p293', 'p294', 'p295', 'p297', 'p298', 'p299', 'p300', 'p301', 'p302', 'p303', 'p304', 'p305', 'p306', 'p307', 'p308', 'p310', 'p311', 'p312', 'p313', 'p314', 'p316', 'p317', 'p318', 'p323', 'p326', 'p329', 'p330', 'p333', 'p334', 'p335', 'p336', 'p339', 'p340', 'p341', 'p343', 'p345', 'p347', 'p351', 'p360', 'p361', 'p362', 'p363', 'p364', 'p374', 'p376']
nspeakers = len(speaker_list) + 100 ## add 100 extra slots for posterity's sake 
speaker_embedding_size = 128


n_utts = 0 ## 0 means use all data, other positive integer means select this many sentences from beginning of training set 
random_reduction_on_the_fly = True ## Randomly choose shift when performing reduction to get coarse features.
            


# signal processing
trim_before_spectrogram_extraction = 0    
vocoder = 'griffin_lim'  
sr = 16000  # Sampling rate.
n_fft = 2048  # fft points (samples)
frame_shift = 0.0125  # seconds
frame_length = 0.05  # seconds
hop_length = int(sr * frame_shift)
win_length = int(sr * frame_length)    
prepro = True  # don't extract spectrograms on the fly
full_dim = n_fft//2+1
n_mels = 80  # Number of Mel banks to generate
power = 1.5  # Exponent for amplifying the predicted magnitude
n_iter = 50  # Number of inversion iterations
preemphasis = .97
max_db = 100
ref_db = 20


# Model
r = 4 # Reduction factor. Do not change this.
dropout_rate = 0.05
e = 128 # == embedding
d = 256 # == hidden units of Text2Mel
c = 512 # == hidden units of SSRN
attention_win_size = 3
g = 0.2 ## determines width of band in attention guide
norm = 'layer' ## type of normalisation layers to use: from ['layer', 'batch', None]

## loss weights : T2M
lw_mel = 0.3333
lw_bd1 = 0.3333
lw_att = 0.3333
##              : SSRN
lw_mag = 0.5
lw_bd2 = 0.5


## validation:
validpatt = '_00' ##  sentence names containing this substring will be held out of training. 
            ## E.g. for LJSpeech, 'LJ050-' will hold out 50th chapter.
            ## For VCTK: '_00' will hold out common sentence set ('please call stella' etc.) for all speakers

validation_sentences_to_evaluate = 32 
validation_sentences_to_synth_params = 16


# training scheme
restart_from_savepath = []
lr = 0.001 # Initial learning rate.
batchsize = {'t2m': 32, 'ssrn': 32}
num_threads = 8 # how many threads get_batch should use to build training batches of data (default: 8)
validate_every_n_epochs = 10   ## how often to compute validation score and save speech parameters
save_every_n_epochs = 0 ## as well as 5 latest models, how often to archive a model
max_epochs = 1000

# attention plotting during training
plot_attention_every_n_epochs = 0 ## set to 0 if you do not wish to plot attention matrices
num_sentences_to_plot_attention = 0 ## number of sentences to plot attention matrices for